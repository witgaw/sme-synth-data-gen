# Polish SME Dataset for RAG Evaluation

A synthetic **Polish-language** dataset of business documents from a fictional marketing agency. Designed for testing and evaluating RAG (Retrieval-Augmented Generation) systems.

> **Language:** All documents are in Polish (polski).

## Overview

The dataset mimics internal documents of Kreatywna Fala Sp. z o.o., a small marketing agency in Wrocław, Poland. Documents span June 2023 - July 2024 and include realistic business scenarios: client negotiations, project crises, team communications, financial reports.

### Key Features

- **116 documents** across multiple types (emails, reports, spreadsheets, presentations, meeting notes, PDFs)
- **SQLite database** with CRM/project data (employees, clients, projects, invoices, time tracking)
- **Polish business language** with authentic terminology and conventions
- **70 evaluation questions** across multiple modalities (documents, OCR, database, multi-hop)
- **Interconnected narratives** testing multi-document reasoning
- **Multi-hop questions** requiring cross-referencing documents, OCR, and database

## Installation

Requires [uv](https://docs.astral.sh/uv/) for dependency management.

```bash
# Clone the repository
git clone https://github.com/your-username/sme-synth-data-gen.git
cd sme-synth-data-gen

# Install dependencies
uv sync
```

## Usage

### Reading the Dataset

The core dataset is in `dataset/documents.json` - no dependencies required:

```python
import json

with open('dataset/documents.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

for doc in data['documents']:
    print(f"{doc['id']}: {doc['type']} - {doc.get('title') or doc.get('subject')}")
```

### Generating Actual Files

```bash
# Generate everything (documents + database + PDFs if deps available)
uv run generate

# For PDF generation, install extra dependencies first:
uv sync --extra pdf

# Exclude specific outputs if needed:
uv run generate --no-pdf         # Skip PDFs
uv run generate --no-db          # Skip database
uv run generate --no-timestamps  # Keep current timestamps (default: set to document dates)
```

### SQLite Database

The generator creates a CRM database (`kreatywna_fala_crm.db`) with 8 tables:

| Table | Description |
|-------|-------------|
| `employees` | Agency staff with hourly rates |
| `clients` | Client companies |
| `contacts` | Contact persons at clients |
| `projects` | Projects and campaigns |
| `project_assignments` | Employee-project assignments |
| `time_entries` | Hours logged |
| `invoices` | Invoices issued |
| `expenses` | Vendor costs |

```bash
# Query example
sqlite3 output/kreatywna_fala_crm.db "SELECT name, hourly_rate FROM employees"
```

### Running Tests

```bash
uv sync --extra dev  # includes pytest, ruff
uv run test
```

### Evaluating RAG System Answers

The repository includes a standardized evaluation script for scoring RAG system outputs.

```bash
# Evaluate your answers against ground truth
uv run evaluate submissions.json

# Output to file instead of stdout
uv run evaluate submissions.json --output results.md

# Get JSON output for programmatic use
uv run evaluate submissions.json --display json
```

**Python API:**
```python
from scripts.evaluate import evaluate

# From file
results = evaluate("submissions.json")

# From dict
results = evaluate({"q001": "15 czerwca 2023", "q002": "45"})

print(results["summary"]["auto_scored_percentage"])  # e.g., 84.5
```

**Input format** (`submissions.json`):
```json
{
  "q001": "15 czerwca 2023",
  "q002": "45",
  "q024": "Relacja z ModaNet zaczęła się dobrze...",
  ...
}
```

A template with all question IDs is available at `dataset/submissions_template.json`.

**Scoring:**
- **Auto-scored** (55 questions): 1.0 = exact match, 0.5 = correct after normalization, 0.0 = wrong
- **Semantic analysis** (12 questions): Qualitative answers with rubrics (0-5 scale with exact MUST/SHOULD item counts)
- **Temporal filter** (3 questions): Precision/recall on document retrieval

## OCR Testing (Optional)

The dataset includes 16 scanned PDF documents for testing OCR capabilities:

| Difficulty | Count | Description |
|------------|-------|-------------|
| Easy | 8 | Clean scans - contracts, invoices, bank confirmations |
| Hard | 8 | Degraded quality - handwritten notes, faxes, crumpled receipts, multi-column layouts |

### Generating PDFs

```bash
# Install PDF generation dependencies
uv sync --extra pdf

# Generate all files including PDFs (PDFs generated by default when deps available)
uv run generate
```

### Hard PDF Challenges

The "hard" PDFs include realistic degradation effects:
- Slight rotation (0.5-2.5 degrees)
- Gaussian noise
- Reduced contrast
- Lower DPI (150-200)
- JPEG compression artifacts

OCR questions in `ground_truth.json` are marked with `"requires_ocr": true` and `"ocr_difficulty": "easy"|"hard"`. Skip these if not testing OCR pipelines.

## Repository Structure

```
├── README.md
├── LICENSE
├── pyproject.toml
│
├── dataset/
│   ├── documents.json           # 116 documents (100 standard + 16 PDF)
│   ├── database.json            # SQLite database schema and data definition
│   ├── ground_truth.json        # 70 evaluation questions
│   ├── qualitative_rubric.json  # Scoring rubrics for narrative questions
│   ├── company_meta.json        # Synthesizable facts + 8 meta-questions
│   ├── submissions_template.json # Empty template for RAG system answers
│   └── submissions_sample.json  # Sample answers for testing the evaluator
│
├── docs/
│   └── verification.md          # Manual verification guide for ground truth
│
├── context/
│   └── company_bible.md         # Company background, team, clients, timeline
│
├── scripts/
│   ├── generate_files.py        # Document generator (entry point: `generate`)
│   ├── generate_database.py     # Database generator (entry point: `generate-db`)
│   └── evaluate.py              # Answer evaluator (entry point: `evaluate`)
│
└── tests/
    └── test_dataset.py          # Dataset validation tests (54 tests)
```

## Dataset Statistics

| Metric | Value |
|--------|-------|
| Total Documents | 116 |
| Standard Documents | 100 |
| OCR Documents (PDF) | 16 (8 easy, 8 hard) |
| Database Tables | 8 |
| Database Rows | 105 |
| Language | Polish |
| Document Types | 8+ (emails, reports, proposals, meeting notes, spreadsheets, presentations, PDFs) |
| Time Range | June 2023 - July 2024 |
| Clients | 8 |
| Team Members | 9 |
| Evaluation Questions | 70 total |

## Evaluation Questions

| Category | Count | Description |
|----------|-------|-------------|
| Exact Match | 19 | Single fact retrieval (dates, amounts, names) |
| Multi-Document Synthesis | 4 | Combine information across documents |
| Qualitative/Narrative | 4 | Complex relationships, scored with rubrics |
| Temporal Filter | 3 | Date-based retrieval |
| Negative | 5 | Absent or out-of-scope information |
| Meta (synthesizable) | 8 | Implicit facts derivable from multiple documents |
| OCR | 16 | Facts in scanned PDFs (8 easy, 8 hard) |
| Multi-hop OCR | 5 | Combine OCR-extracted data with other documents |
| **Database (SQL)** | **10** | Facts queryable from SQLite database |
| **Multi-hop DB+Docs** | **4** | Combine database queries with document search |

### Meta-Questions

The `company_meta.json` file contains questions testing a system's ability to aggregate implicit knowledge:

- Company headcount and organizational structure
- Typical contract sizes and financial patterns
- Common operational issues and resolution approaches
- Implicit company values and communication culture

## Key Narratives

1. **ModaNet Drama:** Scope creep → budget overrun → client complaints → account handover → recovery
2. **Smakosz Crisis:** Delivery delays → payment withheld → resolution meeting → relationship rebuilt
3. **MaszBud Sales:** 10-month B2B cycle → management change → deal closed
4. **Boryna's Growth:** Junior account manager's learning journey

## License

MIT License. See [LICENSE](LICENSE) file.

## Technical Details

- **Generated by:** Claude (Sonnet 4.5 + Opus 4.5)
- **Encoding:** UTF-8
- **Date format:** ISO 8601 with timezone
- **Currency:** PLN (Polish Złoty)

---

*All names, companies, and events are entirely fictional. Any resemblance to real entities is coincidental.*
